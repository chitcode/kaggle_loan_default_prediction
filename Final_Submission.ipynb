{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Author: Chitrasen\n",
      "## \n",
      "## train_master has to be commented after first run to reduce the data loading time.\n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.cross_validation import KFold,StratifiedKFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import roc_auc_score,mean_absolute_error,f1_score,classification_report\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.feature_selection import SelectKBest,f_regression\n",
      "from sklearn.ensemble import GradientBoostingRegressor,ExtraTreesRegressor\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "# following 2 lines has to be commented after first run, to reduce the data loading time for interatively\n",
      "# experimenting with different models.\n",
      "train_master = pd.read_csv('data/train_v2.csv', low_memory=True)\n",
      "train_master = train_master.drop(['id','f53','f137','f206','f276','f419','f466','f639','f703','f731','f743'],1)\n",
      "\n",
      "train = train_master.copy()\n",
      "\n",
      "train_loss = train.loss\n",
      "train = train.drop(['loss'],1)\n",
      "features = train.columns\n",
      "\n",
      "train_featured = train[['f527','f528','f271','f9']]#,'f2', 'f57', 'f9','f39','f271','f274'\n",
      "f2_unique = train_master['f2'].unique()\n",
      "for feature_val in f2_unique:\n",
      "    train_featured['f2_'+str(feature_val)] = 1.* (train_master['f2'] == feature_val)\n",
      "                             \n",
      "imp1 = Imputer()\n",
      "\n",
      "imp1.fit(train_featured)\n",
      "train_featured = imp1.transform(train_featured)\n",
      "\n",
      "scalaer = StandardScaler()\n",
      "scalaer.fit(train_featured)\n",
      "train_featured = scalaer.transform(train_featured)\n",
      "\n",
      "\n",
      "train = train[['f527','f528','f271','f9','f39','f57','f275', u'f13', u'f25', u'f26', u'f31', u'f63', u'f67', \n",
      "               u'f68', u'f142', u'f230', \n",
      "               u'f258', u'f260', u'f263', u'f270', u'f281', u'f282', u'f283', u'f314', \n",
      "               u'f315', u'f322', u'f323', u'f324', u'f376', u'f377', u'f395', u'f396', \n",
      "               u'f397', u'f400', u'f402', u'f404', u'f405', u'f406', u'f424', u'f442', \n",
      "               u'f443', u'f516', u'f517', u'f596', u'f597', u'f598', u'f599', u'f629', \n",
      "               u'f630', u'f631', u'f671', u'f675', u'f676', u'f765', u'f766', u'f767', u'f768']]\n",
      "\n",
      "for feature_val in f2_unique:\n",
      "    train['f2_'+str(feature_val)] = 1.* (train_master['f2'] == feature_val)\n",
      "\n",
      "\n",
      "imp2 = Imputer()\n",
      "imp2.fit(train)\n",
      "train = imp2.transform(train)\n",
      "\n",
      "\n",
      "scalaer = StandardScaler()\n",
      "scalaer.fit(train)\n",
      "train = scalaer.transform(train)\n",
      "\n",
      "\n",
      "##### models\n",
      "clf = LogisticRegression(penalty='l2',C=1e10, dual = False,class_weight ='auto')\n",
      "\n",
      "regr = GradientBoostingRegressor(loss='lad',n_estimators = 200,max_depth = 3,\n",
      "                              min_samples_leaf = 4,min_samples_split=2,learning_rate = 0.1, alpha = 0.9)\n",
      "\n",
      "\n",
      "train_loss_b = train_loss.apply(lambda x:1 if x>0 else 0)\n",
      "\n",
      "auc_score = []\n",
      "mae_score = []\n",
      "f1 = []\n",
      "kf = StratifiedKFold(train_loss,n_folds=4)\n",
      "\n",
      "for i,(train_idx,cv_idx) in enumerate(kf):\n",
      "    print '='*20\n",
      "    X_train = train_featured[train_idx,:]\n",
      "    X_cv = train_featured[cv_idx,:]\n",
      "    \n",
      "    y_train = train_loss_b.values[train_idx]\n",
      "    y_cv = train_loss_b.values[cv_idx]\n",
      "    \n",
      "    #sgd.fit_transform(X_train,y_train)\n",
      "    #X_train = sgd.transform()\n",
      "\n",
      "    clf.fit(X_train,y_train)\n",
      "    \n",
      "    auc = roc_auc_score(y_cv,clf.predict_proba(X_cv)[:,1])    \n",
      "    \n",
      "    auc_score.append(auc)\n",
      "    \n",
      "    #y_clf_pred = clf.predict(X_cv)\n",
      "    y_clf_pred = np.zeros(y_cv.shape[0])\n",
      "    y_clf_pred[clf.predict_proba(X_cv)[:,1] > 0.65] = 1\n",
      "    f1_local = f1_score(y_cv,y_clf_pred)\n",
      "    f1.append(f1_local)\n",
      "    \n",
      "    print 'AUC score for ',i,auc\n",
      "    print 'F1 score for ',i, f1_local\n",
      "    #print classification_report(y_cv,y_clf_pred)\n",
      "    \n",
      "    X_train_reg = train[train_idx,:][y_train == 1,:]\n",
      "    y_train_reg = train_loss.values[train_idx][y_train == 1]\n",
      "    \n",
      "    X_cv_reg = train[cv_idx,:][y_clf_pred == 1,:]\n",
      "    y_cv_reg = train_loss.values[cv_idx][y_clf_pred == 1]\n",
      "    \n",
      "    #selector.fit(X_train_reg,y_train_reg)\n",
      "    #X_train_reg = selector.transform(X_train_reg)\n",
      "    #X_cv_reg = selector.transform(X_cv_reg)    \n",
      "    \n",
      "    #print 'Important Features ::::', features[selector.get_support()]\n",
      "    ##ridge.fit(X_train_reg,y_train_reg)\n",
      "    ##y_reg_pred = ridge.predict(X_cv_reg)\n",
      "    \n",
      "    regr.fit(X_train_reg,y_train_reg)\n",
      "    y_reg_pred = regr.predict(X_cv_reg)\n",
      "    \n",
      "    y_reg_pred[y_reg_pred < 0] = 0\n",
      "    y_reg_pred[y_reg_pred > 100] = 100\n",
      "    print 'Regg MAE',mean_absolute_error(y_reg_pred,y_cv_reg)\n",
      "    \n",
      "    y_clf_pred[y_clf_pred == 1] = y_reg_pred\n",
      "    mae = mean_absolute_error(y_clf_pred,train_loss.values[cv_idx])\n",
      "    print 'MAE for ',i,mae\n",
      "    mae_score.append(mae)\n",
      "    \n",
      "print 'Avg AUC score', np.mean(auc_score)\n",
      "print 'Avg f1 score', np.mean(f1)\n",
      "print 'F1 scores array',f1\n",
      "print 'Avg MAE score',np.mean(mae_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(105471, 14)\n",
        "===================="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 0.988700107593\n",
        "F1 score for  0 0.922722029988\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.8143480075\n",
        "MAE for  0 0.522274844837\n",
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.990300908451\n",
        "F1 score for  1 0.925556408289\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.7991966571\n",
        "MAE for  1 0.522132052243\n",
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.990008316412\n",
        "F1 score for  2 0.923728813559\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.88783585259\n",
        "MAE for  2 0.523664944297\n",
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.988432813939\n",
        "F1 score for  3 0.924702266615\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.95150175619\n",
        "MAE for  3 0.527594961461\n",
        "Avg AUC score 0.989360536599\n",
        "Avg f1 score 0.924177379613\n",
        "F1 scores array [0.92272202998846597, 0.92555640828856489, 0.92372881355932213, 0.92470226661544375]\n",
        "Avg MAE score 0.523916700709\n"
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More Feature selection\n",
      "==="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.cross_validation import train_test_split,KFold\n",
      "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
      "from sklearn.metrics import roc_auc_score,mean_absolute_error,f1_score\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest,f_regression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import GradientBoostingRegressor,AdaBoostClassifier\n",
      "\n",
      "import gc\n",
      "\n",
      "train_master = pd.read_csv('data/train_v2.csv', low_memory=True)\n",
      "train_master = train_master.drop(['id','f53','f137','f206','f276','f419','f466','f639','f703','f731','f743'],1)\n",
      "\n",
      "train = train_master.copy()\n",
      "\n",
      "train_loss = train.loss\n",
      "#train = train.drop(['loss'],1)\n",
      "features = train_master.columns[:-1] #removing the loss column\n",
      "for new_feature in features:\n",
      "    gc.collect()\n",
      "    train = train_master.copy()\n",
      "    train_loss = train.loss\n",
      "    train = train.drop(['loss'],1)\n",
      "    print '='*20\n",
      "    print new_feature\n",
      "    #train_featured = train[np.union1d(['f527','f528','f271','f2','f9'],[new_feature])]#,'f2'  ,'f271','f2'\n",
      "    train_featured = train[np.union1d(['f527','f528','f271','f9'],[new_feature])]#,'f2', 'f57', 'f9','f39'\n",
      "    f2_unique = train_master['f2'].unique()\n",
      "    \n",
      "    for feature_val in f2_unique:\n",
      "        train_featured['f2_'+str(feature_val)] = 1.* (train_master['f2'] == feature_val)\n",
      "        \n",
      "    if(train_featured[new_feature].unique().size < 30):\n",
      "        f_unique = train_featured[new_feature].unique()\n",
      "        for f_val in f_unique:\n",
      "            train_featured['fnew_'+str(f_val)] = 1.* (train_featured[new_feature] == f_val)\n",
      "\n",
      "    \n",
      "    imp = Imputer()    \n",
      "    \n",
      "    imp.fit(train_featured)\n",
      "    train_featured = imp.transform(train_featured)\n",
      "    \n",
      "    scalaer = StandardScaler()\n",
      "    scalaer.fit(train_featured)\n",
      "    train_featured = scalaer.transform(train_featured)\n",
      "    \n",
      "    ##### models\n",
      "    clf = LogisticRegression(penalty='l2',C=1e10,dual = False,class_weight = 'auto')\n",
      "    train_loss_b = train_loss.apply(lambda x:1 if x>0 else 0)\n",
      "     \n",
      "    auc_score = []\n",
      "    mae_score = []\n",
      "    f1 = []\n",
      "    kf = KFold(train.shape[0],n_folds=4)\n",
      "    \n",
      "    for i,(train_idx,cv_idx) in enumerate(kf):\n",
      "        \n",
      "        X_train = train_featured[train_idx,:]\n",
      "        X_cv = train_featured[cv_idx,:]\n",
      "        \n",
      "        y_train = train_loss_b.values[train_idx]\n",
      "        y_cv = train_loss_b.values[cv_idx]\n",
      "        \n",
      "        clf.fit(X_train,y_train)\n",
      "        \n",
      "        auc = roc_auc_score(y_cv,clf.predict_proba(X_cv)[:,1])    \n",
      "        \n",
      "        auc_score.append(auc)\n",
      "        \n",
      "        y_clf_pred = np.zeros(y_cv.shape[0])\n",
      "        y_clf_pred[clf.predict_proba(X_cv)[:,1] > 0.65] = 1\n",
      "        f1_local = f1_score(y_cv,y_clf_pred)\n",
      "        \n",
      "        # slowly increasing the value, we can zero-in on different features which can improve F1 score\n",
      "        if f1_local < 0.925: \n",
      "            train = None\n",
      "            break;\n",
      "        \n",
      "        f1.append(f1_local)\n",
      "        \n",
      "        print 'AUC score for ',i,auc\n",
      "        print 'F1 score for ',i, f1_local\n",
      "        \n",
      "        \n",
      "    if len(f1) > 0:\n",
      "        print 'NEW FEATURE',new_feature\n",
      "        print 'Avg AUC score', np.mean(auc_score)\n",
      "        print 'Avg f1 score', np.mean(f1)\n",
      "        print 'F1 scores array',f1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Modelling for submission\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.cross_validation import train_test_split,KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import roc_auc_score,mean_absolute_error\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest,f_regression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "train = pd.read_csv('data/train_v2.csv',low_memory=True)\n",
      "train_loss = train.loss\n",
      "\n",
      "train_featured = train[['f527','f528','f271','f9']]\n",
      "f2_unique = train['f2'].unique()\n",
      "for feature_val in f2_unique:\n",
      "    train_featured['f2_'+str(feature_val)] = 1.* (train['f2'] == feature_val)\n",
      "\n",
      "\n",
      "imp1 = Imputer()\n",
      "imp1.fit(train_featured)\n",
      "train_featured = imp1.transform(train_featured)\n",
      "\n",
      "\n",
      "scalaer1 = StandardScaler()\n",
      "scalaer1.fit(train_featured)\n",
      "train_featured = scalaer1.transform(train_featured)\n",
      "\n",
      "train = train[['f527','f528','f271','f2','f9','f39','f57', u'f13', u'f25', u'f26', u'f31', u'f63', \n",
      "               u'f67', u'f68', u'f142', u'f230', \n",
      "               u'f258', u'f260', u'f263', u'f270', u'f281', u'f282', u'f283', u'f314', \n",
      "               u'f315', u'f322', u'f323', u'f324', u'f376', u'f377', u'f395', u'f396', \n",
      "               u'f397', u'f400', u'f402', u'f404', u'f405', u'f406', u'f424', u'f442', \n",
      "               u'f443', u'f516', u'f517', u'f596', u'f597', u'f598', u'f599', u'f629', \n",
      "               u'f630', u'f631', u'f671', u'f675', u'f676', u'f765', u'f766', u'f767', u'f768']]\n",
      "imp2 = Imputer()\n",
      "imp2.fit(train)\n",
      "train = imp2.transform(train)\n",
      "\n",
      "scalaer2 = StandardScaler()\n",
      "scalaer2.fit(train)\n",
      "train = scalaer2.transform(train)\n",
      "\n",
      "\n",
      "test = pd.read_csv('data/test_v2.csv',low_memory=True)\n",
      "\n",
      "test_featured = test[['f527','f528','f271','f9']]\n",
      "\n",
      "for feature_val in f2_unique:\n",
      "    test_featured['f2_'+str(feature_val)] = 1.* (test['f2'] == feature_val)\n",
      "\n",
      "\n",
      "test_featured = imp1.transform(test_featured)\n",
      "test_featured = scalaer1.transform(test_featured)\n",
      "\n",
      "\n",
      "test = test[['f527','f528','f271','f2','f9','f39','f57', u'f13', u'f25', u'f26', u'f31', u'f63', u'f67', \n",
      "               u'f68', u'f142', u'f230', \n",
      "               u'f258', u'f260', u'f263', u'f270', u'f281', u'f282', u'f283', u'f314', \n",
      "               u'f315', u'f322', u'f323', u'f324', u'f376', u'f377', u'f395', u'f396', \n",
      "               u'f397', u'f400', u'f402', u'f404', u'f405', u'f406', u'f424', u'f442', \n",
      "               u'f443', u'f516', u'f517', u'f596', u'f597', u'f598', u'f599', u'f629', \n",
      "               u'f630', u'f631', u'f671', u'f675', u'f676', u'f765', u'f766', u'f767', u'f768']]\n",
      "test = imp2.transform(test)\n",
      "test = scalaer2.transform(test)\n",
      "\n",
      "gbr = GradientBoostingRegressor(loss='lad',n_estimators = 200,min_samples_leaf = 6,\n",
      "                                min_samples_split=2,max_features=10)\n",
      "\n",
      "train_loss_b = train_loss.apply(lambda x:1 if x>0 else 0)\n",
      "\n",
      "\n",
      "print 'Predicting classification....'\n",
      "\n",
      "clf = LogisticRegression(penalty='l2',C=1e10, dual = False,class_weight = 'auto')\n",
      "clf.fit(train_featured,train_loss_b.values)\n",
      "    \n",
      "y_clf_pred = np.zeros(test_featured.shape[0])\n",
      "y_clf_pred[clf.predict_proba(test_featured)[:,1] > 0.65] = 1\n",
      "clf.predict(test_featured)\n",
      "    \n",
      "print 'Predicting regression....'\n",
      "    \n",
      "X_train_reg = train[train_loss.values > 0,:]\n",
      "y_train_reg = train_loss.values[train_loss.values > 0]\n",
      "    \n",
      "X_test_reg = test[y_clf_pred == 1,:]\n",
      "\n",
      "\n",
      "gbr.fit(X_train_reg,y_train_reg)\n",
      "y_reg_pred = gbr.predict(X_test_reg)\n",
      "\n",
      "y_reg_pred[y_reg_pred < 1] = 1\n",
      "y_reg_pred[y_reg_pred > 100] = 100\n",
      "    \n",
      "y_clf_pred[y_clf_pred == 1] = y_reg_pred\n",
      "\n",
      "submission = pd.read_csv('data/sampleSubmission.csv')\n",
      "submission['loss'] = y_clf_pred\n",
      "submission.to_csv('submissions/beat_benchmark_gbr_527_528_271_9_2fact_1.csv',index=False)\n",
      "print 'Submission File Created.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting classification....\n",
        "Predicting regression...."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Submission File Created."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Post Release Changes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Author: Chitrasen\n",
      "## \n",
      "## train_master has to be commented after first run to reduce the data loading time.\n",
      "\n",
      "\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.cross_validation import KFold,StratifiedKFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import roc_auc_score,mean_absolute_error,f1_score,classification_report\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.feature_selection import SelectKBest,f_regression\n",
      "from sklearn.ensemble import GradientBoostingRegressor,ExtraTreesRegressor\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "\n",
      "# following 2 lines has to be commented after first run, to reduce the data loading time for interatively\n",
      "# experimenting with different models.\n",
      "#train_master = pd.read_csv('data/train_v2.csv', low_memory=True)\n",
      "#train_master = train_master.drop(['id','f53','f137','f206','f276','f419','f466','f639','f703','f731','f743'],1)\n",
      "\n",
      "train = train_master.copy()\n",
      "\n",
      "train_loss = train.loss\n",
      "train = train.drop(['loss'],1)\n",
      "features = train.columns\n",
      "\n",
      "train_featured = train[['f527','f528','f271','f9']]#,'f2', 'f57', 'f9','f39','f271','f274'\n",
      "f2_unique = train_master['f2'].unique()\n",
      "for feature_val in f2_unique:\n",
      "    train_featured['f2_'+str(feature_val)] = 1.* (train_master['f2'] == feature_val)\n",
      "                             \n",
      "imp1 = Imputer()\n",
      "\n",
      "imp1.fit(train_featured)\n",
      "train_featured = imp1.transform(train_featured)\n",
      "\n",
      "scalaer = StandardScaler()\n",
      "scalaer.fit(train_featured)\n",
      "train_featured = scalaer.transform(train_featured)\n",
      "\n",
      "\n",
      "train = train[['f527','f528','f271','f9','f39','f57','f275', u'f13', u'f25', u'f26', u'f31', u'f63', u'f67', \n",
      "               u'f68', u'f142', u'f230', \n",
      "               u'f258', u'f260', u'f263', u'f270', u'f281', u'f282', u'f283', u'f314', \n",
      "               u'f315', u'f322', u'f323', u'f324', u'f376', u'f377', u'f395', u'f396', \n",
      "               u'f397', u'f400', u'f402', u'f404', u'f405', u'f406', u'f424', u'f442', \n",
      "               u'f443', u'f516', u'f517', u'f596', u'f597', u'f598', u'f599', u'f629', \n",
      "               u'f630', u'f631', u'f671', u'f675', u'f676', u'f765', u'f766', u'f767', u'f768']]\n",
      "\n",
      "for feature_val in f2_unique:\n",
      "    train['f2_'+str(feature_val)] = 1.* (train_master['f2'] == feature_val)\n",
      "\n",
      "\n",
      "imp2 = Imputer()\n",
      "imp2.fit(train)\n",
      "train = imp2.transform(train)\n",
      "\n",
      "\n",
      "scalaer = StandardScaler()\n",
      "scalaer.fit(train)\n",
      "train = scalaer.transform(train)\n",
      "\n",
      "\n",
      "##### models\n",
      "clf = LogisticRegression(penalty='l2',C=1e10, dual = False,class_weight ='auto')\n",
      "\n",
      "regr = GradientBoostingRegressor(loss='lad',n_estimators = 200,max_depth = 3,\n",
      "                              min_samples_leaf = 4,min_samples_split=2,learning_rate = 0.1, alpha = 0.9, subsample = 0.7)\n",
      "\n",
      "\n",
      "train_loss_b = train_loss.apply(lambda x:1 if x>0 else 0)\n",
      "\n",
      "auc_score = []\n",
      "mae_score = []\n",
      "f1 = []\n",
      "kf = StratifiedKFold(train_loss,n_folds=4)\n",
      "\n",
      "for i,(train_idx,cv_idx) in enumerate(kf):\n",
      "    print '='*20\n",
      "    X_train = train_featured[train_idx,:]\n",
      "    X_cv = train_featured[cv_idx,:]\n",
      "    \n",
      "    y_train = train_loss_b.values[train_idx]\n",
      "    y_cv = train_loss_b.values[cv_idx]\n",
      "    \n",
      "    #sgd.fit_transform(X_train,y_train)\n",
      "    #X_train = sgd.transform()\n",
      "\n",
      "    clf.fit(X_train,y_train)\n",
      "    \n",
      "    auc = roc_auc_score(y_cv,clf.predict_proba(X_cv)[:,1])    \n",
      "    \n",
      "    auc_score.append(auc)\n",
      "    \n",
      "    #y_clf_pred = clf.predict(X_cv)\n",
      "    y_clf_pred = np.zeros(y_cv.shape[0])\n",
      "    y_clf_pred[clf.predict_proba(X_cv)[:,1] > 0.65] = 1\n",
      "    f1_local = f1_score(y_cv,y_clf_pred)\n",
      "    f1.append(f1_local)\n",
      "    \n",
      "    print 'AUC score for ',i,auc\n",
      "    print 'F1 score for ',i, f1_local\n",
      "    #print classification_report(y_cv,y_clf_pred)\n",
      "    \n",
      "    X_train_reg = train[train_idx,:][y_train == 1,:]\n",
      "    y_train_reg = train_loss.values[train_idx][y_train == 1]\n",
      "    \n",
      "    X_cv_reg = train[cv_idx,:][y_clf_pred == 1,:]\n",
      "    y_cv_reg = train_loss.values[cv_idx][y_clf_pred == 1]\n",
      "    \n",
      "    #selector.fit(X_train_reg,y_train_reg)\n",
      "    #X_train_reg = selector.transform(X_train_reg)\n",
      "    #X_cv_reg = selector.transform(X_cv_reg)    \n",
      "    \n",
      "    #print 'Important Features ::::', features[selector.get_support()]\n",
      "    ##ridge.fit(X_train_reg,y_train_reg)\n",
      "    ##y_reg_pred = ridge.predict(X_cv_reg)\n",
      "    \n",
      "    regr.fit(X_train_reg,np.log(y_train_reg))\n",
      "    y_reg_pred = np.exp(regr.predict(X_cv_reg))\n",
      "    \n",
      "    y_reg_pred[y_reg_pred < 0] = 0\n",
      "    y_reg_pred[y_reg_pred > 100] = 100\n",
      "    print 'Regg MAE',mean_absolute_error(y_reg_pred,y_cv_reg)\n",
      "    \n",
      "    y_clf_pred[y_clf_pred == 1] = y_reg_pred\n",
      "    mae = mean_absolute_error(y_clf_pred,train_loss.values[cv_idx])\n",
      "    print 'MAE for ',i,mae\n",
      "    mae_score.append(mae)\n",
      "    \n",
      "print 'Avg AUC score', np.mean(auc_score)\n",
      "print 'Avg f1 score', np.mean(f1)\n",
      "print 'F1 scores array',f1\n",
      "print 'Avg MAE score',np.mean(mae_score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 0.988700517757\n",
        "F1 score for  0 0.922722029988\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.67271443463\n",
        "MAE for  0 0.507471214421\n",
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 0.990300823\n",
        "F1 score for  1 0.925556408289\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.66762914899\n",
        "MAE for  1 0.508330636609\n",
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 0.990007666986\n",
        "F1 score for  2 0.92355093395\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.76612222369\n",
        "MAE for  2 0.511170272621\n",
        "====================\n",
        "AUC score for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3 0.988437943083\n",
        "F1 score for  3 0.924524678318\n",
        "Regg MAE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.81075447298\n",
        "MAE for  3 0.513039172237\n",
        "Avg AUC score 0.989361737707\n",
        "Avg f1 score 0.924088512636\n",
        "F1 scores array [0.92272202998846597, 0.92555640828856489, 0.92355093394954757, 0.92452467831764928]\n",
        "Avg MAE score 0.510002823972\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import Imputer\n",
      "from sklearn.cross_validation import train_test_split,KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import roc_auc_score,mean_absolute_error\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "from sklearn.feature_selection import SelectKBest,f_regression\n",
      "from sklearn.linear_model import Ridge\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "\n",
      "train = pd.read_csv('data/train_v2.csv',low_memory=True)\n",
      "train_loss = train.loss\n",
      "\n",
      "train_featured = train[['f527','f528','f271','f9']]\n",
      "f2_unique = train['f2'].unique()\n",
      "for feature_val in f2_unique:\n",
      "    train_featured['f2_'+str(feature_val)] = 1.* (train['f2'] == feature_val)\n",
      "\n",
      "\n",
      "imp1 = Imputer()\n",
      "imp1.fit(train_featured)\n",
      "train_featured = imp1.transform(train_featured)\n",
      "\n",
      "\n",
      "scalaer1 = StandardScaler()\n",
      "scalaer1.fit(train_featured)\n",
      "train_featured = scalaer1.transform(train_featured)\n",
      "\n",
      "train = train[['f527','f528','f271','f2','f9','f39','f57', u'f13', u'f25', u'f26', u'f31', u'f63', \n",
      "               u'f67', u'f68', u'f142', u'f230', \n",
      "               u'f258', u'f260', u'f263', u'f270', u'f281', u'f282', u'f283', u'f314', \n",
      "               u'f315', u'f322', u'f323', u'f324', u'f376', u'f377', u'f395', u'f396', \n",
      "               u'f397', u'f400', u'f402', u'f404', u'f405', u'f406', u'f424', u'f442', \n",
      "               u'f443', u'f516', u'f517', u'f596', u'f597', u'f598', u'f599', u'f629', \n",
      "               u'f630', u'f631', u'f671', u'f675', u'f676', u'f765', u'f766', u'f767', u'f768']]\n",
      "imp2 = Imputer()\n",
      "imp2.fit(train)\n",
      "train = imp2.transform(train)\n",
      "\n",
      "scalaer2 = StandardScaler()\n",
      "scalaer2.fit(train)\n",
      "train = scalaer2.transform(train)\n",
      "\n",
      "\n",
      "test = pd.read_csv('data/test_v2.csv',low_memory=True)\n",
      "\n",
      "test_featured = test[['f527','f528','f271','f9']]\n",
      "\n",
      "for feature_val in f2_unique:\n",
      "    test_featured['f2_'+str(feature_val)] = 1.* (test['f2'] == feature_val)\n",
      "\n",
      "\n",
      "test_featured = imp1.transform(test_featured)\n",
      "test_featured = scalaer1.transform(test_featured)\n",
      "\n",
      "\n",
      "test = test[['f527','f528','f271','f2','f9','f39','f57', u'f13', u'f25', u'f26', u'f31', u'f63', u'f67', \n",
      "               u'f68', u'f142', u'f230', \n",
      "               u'f258', u'f260', u'f263', u'f270', u'f281', u'f282', u'f283', u'f314', \n",
      "               u'f315', u'f322', u'f323', u'f324', u'f376', u'f377', u'f395', u'f396', \n",
      "               u'f397', u'f400', u'f402', u'f404', u'f405', u'f406', u'f424', u'f442', \n",
      "               u'f443', u'f516', u'f517', u'f596', u'f597', u'f598', u'f599', u'f629', \n",
      "               u'f630', u'f631', u'f671', u'f675', u'f676', u'f765', u'f766', u'f767', u'f768']]\n",
      "test = imp2.transform(test)\n",
      "test = scalaer2.transform(test)\n",
      "\n",
      "gbr = GradientBoostingRegressor(loss='lad',n_estimators = 200,min_samples_leaf = 6,\n",
      "                                min_samples_split=2,max_features=10)\n",
      "\n",
      "train_loss_b = train_loss.apply(lambda x:1 if x>0 else 0)\n",
      "\n",
      "\n",
      "print 'Predicting classification....'\n",
      "\n",
      "clf = LogisticRegression(penalty='l2',C=1e10, dual = False,class_weight = 'auto')\n",
      "clf.fit(train_featured,train_loss_b.values)\n",
      "    \n",
      "y_clf_pred = np.zeros(test_featured.shape[0])\n",
      "y_clf_pred[clf.predict_proba(test_featured)[:,1] > 0.65] = 1\n",
      "clf.predict(test_featured)\n",
      "    \n",
      "print 'Predicting regression....'\n",
      "    \n",
      "X_train_reg = train[train_loss.values > 0,:]\n",
      "y_train_reg = train_loss.values[train_loss.values > 0]\n",
      "    \n",
      "X_test_reg = test[y_clf_pred == 1,:]\n",
      "\n",
      "\n",
      "gbr.fit(X_train_reg,np.log(y_train_reg))\n",
      "y_reg_pred = np.exp(gbr.predict(X_test_reg))\n",
      "\n",
      "y_reg_pred[y_reg_pred < 1] = 1\n",
      "y_reg_pred[y_reg_pred > 100] = 100\n",
      "    \n",
      "y_clf_pred[y_clf_pred == 1] = y_reg_pred\n",
      "\n",
      "submission = pd.read_csv('data/sampleSubmission.csv')\n",
      "submission['loss'] = y_clf_pred\n",
      "submission.to_csv('submissions/beat_benchmark_gbr_withlogloss.csv',index=False)\n",
      "print 'Submission File Created.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting classification....\n",
        "Predicting regression...."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Submission File Created."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}